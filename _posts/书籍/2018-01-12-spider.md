---
layout: blog
banana: true
category: spider
title:  Python爬虫实战演练
date:   2018-1-12 03:06:00
background-image: https://i.loli.net/2018/01/12/5a57b5f1eefc3.jpg

tags:
- MD
- spider
- python
---
>> 看似教程实则复习

# Python爬虫实战演练

## 目录
### 1.准备条件
### 2.演习开始
### 3.请观赏

## 1.准备条件
### 1.1 Python2.7
### 1.2 爬取目标：qingbuyaohaixiu.com

## 2.演习开始

记住，万物皆对象，要养成把事情解构成一个个对象之间的关系。
所以，我们先创建一个对象

`class spider():`

每一个对象都会有一个解析函数用于初始化一些数据。
这里初始化两个变量，需要爬的网址以及浏览器标识。

    def __init__(self):
        self.url="http://qingbuyaohaixiu.com/page/"
        self.header="User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"

接着再写一个获取单个网页中图片链接的函数
这里有个重点：定位所需内容的正则表达式
![](https://i.loli.net/2018/01/12/5a57a4ef332c9.png
)
如图，所需图片位于层层div下，通过正则我们可以精准定位到所需内容的位置。
>> 非贪婪算法 :(.*?)尽可能少的匹配。

可以看到上层有一个article的div包围，然后我们需要得到的图片链接前是src,使用非贪婪匹配。

`<.*?article.*?src="(.*?)"class="attachment.*?/a>.*?/div>`

    def spiderget(self,i):      #传入一个形参用于循环获取网址
        html=requests.get(self.url,self.header).text
        self.items = re.findall('<.*?article.*?src="(.*?)" class="attachment.*?/a>.*?/div>', html, re.S)

测试一下是否获取到了我们所需的网址。
![](https://i.loli.net/2018/01/12/5a57a7dba9c00.png)
没毛病，我们得到了需要的图片网址。

然后是把得到的网址再解析成图片，保持到文件夹。

    def spiderw(self,i):
        self.spiderget()
        j=0
        for item in self.items:
            pic=requests.get(item,timeout=10)   #获取图片
            spring='pic\\'+str(i)+str(j)+'.jpg' #写入图片文件名
            fo=open(string,'wb' )               #以只写模式打开文件链接
            fo.write(pic.content)               #写入文件
            fo.close()                          #关闭文件链接
            j+=1

最后是在主函数

    if __name__ == "__main__":
        spider=spider()
        try:
            for i in range(1):
                spider.spiderw(i)
        except:
            raise
## 3.请观赏

运行看看
![](https://i.loli.net/2018/01/12/5a57ac85a2434.png
)
一页十张，没问题。
因为在主函数里我们就测试了1页，windows系统跑的有点慢，上传到服务器跑跑

` scp -P 3210 本地路径\sqider.py  root@服务器ip:上传路径`

这次在服务器上我们把整个网站图片都爬下来，把i设为网站最大页数+1
最终效果图（终端下）
![](https://i.loli.net/2018/01/12/5a57ae700fa9b.png
)

最终效果图（私有云web）
![](https://i.loli.net/2018/01/12/5a57ae1971933.png
)

好了，可以补充营养快线了。XDDDDDDD

***

贴下全代码

    # -*-coding:utf-8-*-
    import re
    import requests

    class spider():
        def __init__(self):
            self.url="http://qingbuyaohaixiu.com/page/"
            self.header="User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"

        def spiderget(self,i):
            html = requests.get(self.url+str(i),self.header).text
            self.items = re.findall('<.*?article.*?src="(.*?)" class="attachment.*?/a>.*?/div>', html, re.S)
            return self.items

        def spiderw(self,i):
            self.sqidergo(i)
            j=0
            for item in self.items:
                    pic = requests.get(item, timeout=10)
                    sring = 'pic\\' + str(i) +str(j)+ '.jpg'  
                    fq = open(sring, 'wb')  
                    fq.write(pic.content)  
                    fq.close() 
                    j+=1

    if __name__ == "__main__":
        spider = spider()
        try:
            for i in range(120)
                spider.spiderw()
        except:
            raise

对象内函数/方法变量不能互相调用，需要self对象化后，通过self来调用。

***

写给最后的话：
* 1.本次代码只是基础爬取目录图片，但爬虫思路大同小异，以二两拨千斤。
* 2.爬取别人网站造成无意义流量其实是不道德的行为，本教程为学习所需，仅仅爬取目录小图片，全部使用流量不超过200M，。
